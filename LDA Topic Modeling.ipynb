{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12aa3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000f5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_AfterClove = pd.read_csv(r\"C:\\Users\\saisa\\OneDrive - Arizona State University\\Documents\\Courseworks\\CIS 518 Big Data\\Group Project\\llm_predictions-before-clove.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b3233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for positive and negative labels\n",
    "positive_sentences_BeforeClove = Predictions_BeforeClove[Predictions_BeforeClove['Label'].str.contains('positive', case=False, na=False)]['Sentence']\n",
    "\n",
    "negative_sentences_BeforeClove = Predictions_BeforeClove[Predictions_BeforeClove['Label'].str.contains('negative', case=False, na=False)]['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d9f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(message):\n",
    "    # Generating the list of words in the message\n",
    "    def form_sentence(message):\n",
    "        message = message.lower()\n",
    "        message_blob = TextBlob(message)\n",
    "        return ' '.join(message_blob.words)\n",
    "    \n",
    "    new_message = form_sentence(message)\n",
    "    \n",
    "    # Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(message):\n",
    "        message_list = [item for item in message.split()]\n",
    "        clean_words = [word for word in message_list if re.match(r'[^\\W\\d]*$', word)]\n",
    "        clean_sentence = ' '.join(clean_words)\n",
    "        clean_mess = [stopword for stopword in clean_sentence.split() if stopword not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    \n",
    "    no_punc_message = no_user_alpha(new_message)\n",
    "    \n",
    "    # Normalizing the words in messages\n",
    "    def normalization(message_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_message = []\n",
    "        for word in message_list:\n",
    "            if word.lower() == 'clove':\n",
    "                normalized_message.append('clove')\n",
    "            else:\n",
    "                normalized_text = lem.lemmatize(word, 'v')\n",
    "                normalized_message.append(normalized_text)\n",
    "        return ' '.join(normalized_message)  # Join words into a single string\n",
    "    \n",
    "    return normalization(no_punc_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be6dc113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing with safety checks\n",
    "def safe_text_processing(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    return text_processing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf98166",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentences_preprocessed = positive_sentences_BeforeClove.apply(safe_text_processing)\n",
    "negative_sentences_preprocessed = negative_sentences_BeforeClove.apply(safe_text_processing)\n",
    "\n",
    "# Add preprocessed text to DataFrame\n",
    "Predictions_BeforeClove.loc[Predictions_BeforeClove['Label'].str.contains('positive', case=False, na=False), 'Preprocessed_Sentence'] = positive_sentences_preprocessed\n",
    "Predictions_BeforeClove.loc[Predictions_BeforeClove['Label'].str.contains('negative', case=False, na=False), 'Preprocessed_Sentence'] = negative_sentences_preprocessed\n",
    "\n",
    "# Filter the dataframe for preprocessed sentences\n",
    "positive_sentences_BeforeClove = Predictions_BeforeClove[Predictions_BeforeClove['Label'].str.contains('positive', case=False, na=False)]['Preprocessed_Sentence']\n",
    "negative_sentences_BeforeClove = Predictions_BeforeClove[Predictions_BeforeClove['Label'].str.contains('negative', case=False, na=False)]['Preprocessed_Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3856aae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     completely agree lock another smoke support cl...\n",
       "4     worry last week use time unlock clove test cus...\n",
       "5                      clove main comp hours click head\n",
       "8     interest take upon see agent zellsis say defin...\n",
       "12    work incredibly well viper attack throw usual ...\n",
       "Name: Preprocessed_Sentence, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sentences_BeforeClove.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db80069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sorry anyone want see actual clove footage can...\n",
       "1      open spoil post say footage clove footage clove\n",
       "3    saw concepts cloves hair like damn go one wors...\n",
       "6    personally still plan stick omen trust plat te...\n",
       "7    dont think viable pro play agent stall retake ...\n",
       "Name: Preprocessed_Sentence, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sentences_BeforeClove.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a28090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipelines for positive and negative labels\n",
    "positive_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_df=0.8, min_df=2, stop_words='english')),\n",
    "    ('lda', LatentDirichletAllocation(n_components=10, random_state=42))\n",
    "])\n",
    "\n",
    "negative_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_df=0.8, min_df=2, stop_words='english')),\n",
    "    ('lda', LatentDirichletAllocation(n_components=10, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69935f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipelines\n",
    "positive_pipeline.fit(positive_sentences_BeforeClove.values.astype('U'))\n",
    "negative_pipeline.fit(negative_sentences_BeforeClove.values.astype('U'))\n",
    "\n",
    "# Get the fitted LDA models and vectorizers from the pipelines\n",
    "positive_lda_model = positive_pipeline.named_steps['lda']\n",
    "positive_vectorizer = positive_pipeline.named_steps['vectorizer']\n",
    "\n",
    "negative_lda_model = negative_pipeline.named_steps['lda']\n",
    "negative_vectorizer = negative_pipeline.named_steps['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d06b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the top words for each topic\n",
    "def display_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Top 10 words for topic #{topic_idx}:\")\n",
    "        print([feature_names[i] for i in topic.argsort()[-n_top_words:]])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f955be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most \"Important\" words for forming topic distribution in Positive Labels Before_Clove:\n",
      "['week', 'pronouns', 'make', 'think', 'like', 'smoke', 'use', 'non', 'binary', 'play']\n",
      "\n",
      "Top 10 Topics in Positive Labels Before_Clove:\n",
      "Top 10 words for topic #0:\n",
      "['week', 'pronouns', 'make', 'think', 'like', 'smoke', 'use', 'non', 'binary', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['extra', 'say', 'idk', 'playstyle', 'overheal', 'reyna', 'time', 'really', 'play', 'need']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['multiple', 'shoot', 'omen', 'site', 'jett', 'say', 'think', 'refer', 'smoke', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['base', 'agents', 'site', 'agent', 'think', 'roles', 'come', 'shoot', 'smoke', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['free', 'attack', 'agent', 'extra', 'throw', 'lurk', 'death', 'team', 'smoke', 'viper']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['try', 'probably', 'main', 'ult', 'point', 'people', 'players', 'role', 'controller', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['far', 'pronouns', 'know', 'play', 'die', 'pretty', 'character', 'confuse', 'use', 'agent']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['main', 'release', 'comp', 'brim', 'play', 'week', 'game', 'say', 'agent', 'think']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['new', 'release', 'agent', 'weeks', 'gon', 'na', 'game', 'omen', 'main', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['like', 'team', 'die', 'im', 'double', 'smoke', 'comp', 'controller', 'duelist', 'play']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top topics for positive labels\n",
    "print( \"Most \\\"Important\\\" words for forming topic distribution in Positive Labels Before_Clove:\")\n",
    "positive_top_words = [positive_vectorizer.get_feature_names_out()[i] for i in positive_lda_model.components_[0].argsort()[-10:]]\n",
    "print(positive_top_words)\n",
    "\n",
    "# Display top topics for positive labels\n",
    "print(\"\\nTop 10 Topics in Positive Labels Before_Clove:\")\n",
    "display_top_words(positive_lda_model, positive_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92016b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most \"Important\" words for forming topic distribution in Negative Labels Before_Clove:\n",
      "['agree', 'useless', 'die', 'like', 'actual', 'want', 'people', 'footage', 'say', 'wait']\n",
      "\n",
      "Top 10 Topics in Negative Labels Before_Clove:\n",
      "Top 10 words for topic #0:\n",
      "['agree', 'useless', 'die', 'like', 'actual', 'want', 'people', 'footage', 'say', 'wait']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['footage', 'comms', 'use', 'team', 'actual', 'want', 'retake', 'pronouns', 'multiple', 'say']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['agree', 'useless', 'die', 'like', 'actual', 'want', 'people', 'footage', 'say', 'wait']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['death', 'useless', 'run', 'agent', 'limit', 'pretty', 'util', 'op', 'think', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['footage', 'say', 'wait', 'util', 'players', 'meta', 'hype', 'strong', 'duelist', 'fuck']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['agree', 'useless', 'die', 'like', 'actual', 'want', 'people', 'footage', 'say', 'wait']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['say', 'people', 'players', 'na', 'gon', 'ult', 'try', 'think', 'footage', 'duelist']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['death', 'use', 'die', 'second', 'omen', 'cloves', 'play', 'like', 'limit', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['actual', 'want', 'make', 'person', 'gain', 'feel', 'game', 'doesnt', 'play', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['character', 'wait', 'use', 'team', 'agree', 'fps', 'comms', 'make', 'people', 'game']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top topics for positive labels\n",
    "print( \"Most \\\"Important\\\" words for forming topic distribution in Negative Labels Before_Clove:\")\n",
    "Negative_top_words = [negative_vectorizer.get_feature_names_out()[i] for i in negative_lda_model.components_[0].argsort()[-10:]]\n",
    "print(Negative_top_words)\n",
    "\n",
    "# Display top topics for negative labels\n",
    "print(\"\\nTop 10 Topics in Negative Labels Before_Clove:\")\n",
    "display_top_words(negative_lda_model, negative_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1574241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_AfterClove = pd.read_csv(r\"C:\\Users\\saisa\\OneDrive - Arizona State University\\Documents\\Courseworks\\CIS 518 Big Data\\Group Project\\llm_predictions-after-clove.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d20d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for positive and negative labels\n",
    "positive_sentences_AfterClove = Predictions_AfterClove[Predictions_AfterClove['Label'].str.contains('positive', case=False, na=False)]['Sentence']\n",
    "negative_sentences_AfterClove = Predictions_AfterClove[Predictions_AfterClove['Label'].str.contains('negative', case=False, na=False)]['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35be0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentences_preprocessed = positive_sentences_AfterClove.apply(safe_text_processing)\n",
    "negative_sentences_preprocessed = negative_sentences_AfterClove.apply(safe_text_processing)\n",
    "\n",
    "# Add preprocessed text to DataFrame\n",
    "Predictions_AfterClove.loc[Predictions_AfterClove['Label'].str.contains('positive', case=False, na=False), 'Preprocessed_Sentence'] = positive_sentences_preprocessed\n",
    "Predictions_AfterClove.loc[Predictions_AfterClove['Label'].str.contains('negative', case=False, na=False), 'Preprocessed_Sentence'] = negative_sentences_preprocessed\n",
    "\n",
    "# Filter the dataframe for preprocessed sentences\n",
    "positive_sentences_AfterClove = Predictions_AfterClove[Predictions_AfterClove['Label'].str.contains('positive', case=False, na=False)]['Preprocessed_Sentence']\n",
    "negative_sentences_AfterClove = Predictions_AfterClove[Predictions_AfterClove['Label'].str.contains('negative', case=False, na=False)]['Preprocessed_Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e29ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipelines\n",
    "positive_pipeline.fit(positive_sentences_AfterClove.values.astype('U'))\n",
    "negative_pipeline.fit(negative_sentences_AfterClove.values.astype('U'))\n",
    "\n",
    "# Get the fitted LDA models and vectorizers from the pipelines\n",
    "positive_lda_model = positive_pipeline.named_steps['lda']\n",
    "positive_vectorizer = positive_pipeline.named_steps['vectorizer']\n",
    "\n",
    "negative_lda_model = negative_pipeline.named_steps['lda']\n",
    "negative_vectorizer = negative_pipeline.named_steps['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f583e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most \"Important\" words for forming topic distribution in Positive Labels After_Clove:\n",
      "['really', 'die', 'ult', 'heal', 'know', 'play', 'people', 'sage', 'round', 'kill']\n",
      "\n",
      "Top 10 Topics in Positive Labels After_Clove:\n",
      "Top 10 words for topic #0:\n",
      "['really', 'die', 'ult', 'heal', 'know', 'play', 'people', 'sage', 'round', 'kill']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['death', 'use', 'try', 'entry', 'team', 'better', 'duelist', 'die', 'smoke', 'say']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['correct', 'person', 'control', 'identify', 'birth', 'nonbinary', 'like', 'use', 'assign', 'make']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['agent', 'round', 'agents', 'ability', 'team', 'like', 'play', 'good', 'omen', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['self', 'better', 'omen', 'ult', 'second', 'probably', 'revive', 'team', 'play', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['feel', 'way', 'na', 'gon', 'im', 'ppl', 'game', 'people', 'like', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['use', 'ult', 'smoke', 'good', 'isnt', 'like', 'sec', 'gender', 'really', 'cloves']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['like', 'use', 'good', 'dont', 'say', 'game', 'team', 'think', 'play', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['viper', 'say', 'good', 'agent', 'ult', 'people', 'think', 'pronouns', 'use', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['main', 'think', 'team', 'viper', 'agent', 'smoke', 'controller', 'omen', 'like', 'play']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top topics for positive labels\n",
    "print( \"Most \\\"Important\\\" words for forming topic distribution in Positive Labels After_Clove:\")\n",
    "positive_top_words = [positive_vectorizer.get_feature_names_out()[i] for i in positive_lda_model.components_[0].argsort()[-10:]]\n",
    "print(positive_top_words)\n",
    "\n",
    "# Display top topics for positive labels\n",
    "print(\"\\nTop 10 Topics in Positive Labels After_Clove:\")\n",
    "display_top_words(positive_lda_model, positive_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d3c9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most \"Important\" words for forming topic distribution in Negative Labels After_Clove:\n",
      "['people', 'chamber', 'duelist', 'im', 'play', 'omen', 'let', 'theyre', 'like', 'smoke']\n",
      "\n",
      "Top 10 Topics in Negative Labels After_Clove:\n",
      "Top 10 words for topic #0:\n",
      "['people', 'chamber', 'duelist', 'im', 'play', 'omen', 'let', 'theyre', 'like', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['game', 'map', 'life', 'ur', 'smoke', 'like', 'omen', 'dont', 'think', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['need', 'reyna', 'correct', 'game', 'try', 'say', 'like', 'use', 'play', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['think', 'cloves', 'pronouns', 'game', 'character', 'say', 'like', 'fuck', 'people', 'feel']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['way', 'cloves', 'harm', 'kill', 'like', 'game', 'smoke', 'say', 'think', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['omen', 'pro', 'pick', 'good', 'like', 'agent', 'smoke', 'agents', 'team', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['play', 'bug', 'round', 'result', 'die', 'game', 'kill', 'ultimate', 'use', 'ult']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['like', 'best', 'double', 'refer', 'good', 'death', 'play', 'controller', 'use', 'smoke']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['video', 'https', 'switch', 'probably', 'team', 'type', 'think', 'result', 'player', 'game']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['language', 'thing', 'make', 'im', 'say', 'pronouns', 'think', 'people', 'time', 'use']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top topics for positive labels\n",
    "print( \"Most \\\"Important\\\" words for forming topic distribution in Negative Labels After_Clove:\")\n",
    "Negative_top_words = [negative_vectorizer.get_feature_names_out()[i] for i in negative_lda_model.components_[0].argsort()[-10:]]\n",
    "print(Negative_top_words)\n",
    "\n",
    "# Display top topics for negative labels\n",
    "print(\"\\nTop 10 Topics in Negative Labels After_Clove:\")\n",
    "display_top_words(negative_lda_model, negative_vectorizer.get_feature_names_out())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
