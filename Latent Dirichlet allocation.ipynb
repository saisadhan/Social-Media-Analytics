{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fa3a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most \"Important\" words for forming topic distribution\n",
      "time\n",
      "sen\n",
      "play\n",
      "like\n",
      "game\n",
      "na\n",
      "win\n",
      "make\n",
      "best\n",
      "team\n",
      "Top 10 words for topic #0:\n",
      "['time', 'sen', 'play', 'like', 'game', 'na', 'win', 'make', 'best', 'team']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['better', 'think', 'map', 'win', 'team', 'lose', 'game', 'say', 'sen', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['coach', 'man', 'good', 'dont', 'really', 'yay', 'post', 'people', 'lol', 'like']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['prx', 'way', 'thats', 'team', 'fuck', 'valorant', 'like', 'game', 'play', 'watch']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['say', 'season', 'like', 'think', 'ult', 'better', 'smoke', 'good', 'play', 'team']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['nrg', 'champ', 'sen', 'win', 'time', 'think', 'players', 'year', 'like', 'team']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['hes', 'sen', 'dont', 'list', 'event', 'johnqt', 'look', 'good', 'like', 'think']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['really', 'want', 'master', 'make', 'like', 'roster', 'think', 'say', 'https', 'monyet']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['make', 'map', 'good', 'game', 'dont', 'cleave', 'use', 'like', 'think', 'play']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['champ', 'game', 'loud', 'know', 'people', 'sen', 'say', 'team', 'play', 'like']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3243, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd #Provides text processing capabilities\n",
    "import numpy as np #Provides Python with better math processing capabilities\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#The next line of code reads your Reddit data into this program's memory\n",
    "#Place your reddit data into the same directory of this script and change the below filename\n",
    "reviews_datasets = pd.read_csv(r'C:\\Users\\saisa\\Downloads\\CIS 518 Big Data\\Group Project\\processed_text.csv')\n",
    "\n",
    "reviews_datasets = reviews_datasets.head(20000) #The 20,000 number listed as a parameter here is a limitor of how many records you want to analyze. Adjust this number according to the size of your dataset and whether you run into memory limitations\n",
    "reviews_datasets.dropna() #Drops any records that have a missing value\n",
    "\n",
    "reviews_datasets.head() #Print first 5 rows to console inspect data \n",
    "\n",
    "#This specifies which column to extract for text analysis. It is referenced again a few lines from this comment (doc_term_matrix = count_vect...)\n",
    "reviews_datasets['MsgBody'][10]\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english') #Hyperparameters; max_df = maximum document frequency; min_df = minimum document frequency, stop words = 'english')\n",
    "doc_term_matrix = count_vect.fit_transform(reviews_datasets['MsgBody'].values.astype('U')) #Create document-term matrix\n",
    "doc_term_matrix\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation #Import LDA\n",
    "\n",
    "#n_components is how many topics you want to generate. \n",
    "#This is one of the \"hyperparameters\" for LDA\n",
    "#Many machine learning models have similar hyperparameters\n",
    "#You can adjust hyperparameters to tune model performance\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42) #n_components = number of topics to generate; random_state = a seed to produce reproducible results\n",
    "#More documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "LDA.fit(doc_term_matrix)\n",
    "\n",
    "first_topic = LDA.components_[0]\n",
    "\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "       \n",
    "#Prints out the most \"important\" words for forming topic distribution     \n",
    "print(\"Most \\\"Important\\\" words for forming topic distribution\")  \n",
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names_out()[i])\n",
    "    \n",
    "\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n",
    "    \n",
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068f525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
